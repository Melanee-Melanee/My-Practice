{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LxHgdH2fZ4ST"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Kyl736DDiMxY",
    "outputId": "d2773ee1-583e-46b8-f787-6a81ba404039"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ufeff\"well, prince, so genoa and lucca are now just family estates of the\\nbuonapartes. but i warn you, i'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = 'warpeace_input.txt'\n",
    "raw_text = open(training_file, 'r').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcl7dlcvjGi5",
    "outputId": "f4f5b16c-eec5-45e6-b647-2f7d4eeefaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 3196213\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "print('Total characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKkKMozpjrQx",
    "outputId": "2e76f8ae-af53-4a25-f66a-a038dd78a0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary (unique characters): 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print('Total vocabulary (unique characters): {}'.format(n_vocab))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2Tq12JnuCaW",
    "outputId": "e158dd7f-5a46-4642-d98d-6723632fc4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\\ufeff': 56}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nPax3BuXv1Ut"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seq_length = 100\n",
    "n_seq = int(n_chars / seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "49UE6EgMwkT-"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((n_seq, seq_length, n_vocab))\n",
    "Y = np.zeros((n_seq, seq_length, n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IWickldyw3-9"
   },
   "outputs": [],
   "source": [
    "for i in range(n_seq):\n",
    "\tx_sequence = raw_text[i * seq_length : (i + 1) * seq_length]\n",
    "\tx_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "\tfor j in range(seq_length):\n",
    "\t\tchar = x_sequence[j]\n",
    "\t\tindex = char_to_index[char]\n",
    "\t\tx_sequence_ohe[j][index] = 1.\n",
    "\tX[i] = x_sequence_ohe\n",
    "\ty_sequence = raw_text[i * seq_length + 1 : (i + 1) * seq_length + 1]\n",
    "\ty_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "\tfor j in range(seq_length):\n",
    "\t\tchar = y_sequence[j]\n",
    "\t\tindex = char_to_index[char]\n",
    "\t\ty_sequence_ohe[j][index] = 1.\n",
    "\tY[i] = y_sequence_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-aF4fiayCi2",
    "outputId": "8dacedd0-80cb-483d-ca9a-4a0cf2c8c86b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 100, 57)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "st9QO4fSyT0z",
    "outputId": "007b94d4-a0b7-4b9a-f337-dd66404f561f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 100, 57)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlFVU2ljzO5L",
    "outputId": "f519f705-c102-4e6e-e027-de7685f2e8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rls7BpbEaV_4",
    "outputId": "a6d1322c-ae63-4538-8265-31c7ce5d3078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 3196213\n",
      "Total vocabulary (unique characters): 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\\ufeff']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\\ufeff': 56}\n"
     ]
    }
   ],
   "source": [
    "training_file = 'warpeace_input.txt'\n",
    "\n",
    "raw_text = open(training_file, 'r').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text[:100]\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print('Total characters: {}'.format(n_chars))\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print('Total vocabulary (unique characters): {}'.format(n_vocab))\n",
    "print(chars)\n",
    "\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_index)\n",
    "\n",
    "\n",
    "seq_length = 100\n",
    "n_seq = int(n_chars / seq_length)\n",
    "\n",
    "X = np.zeros((n_seq, seq_length, n_vocab))\n",
    "Y = np.zeros((n_seq, seq_length, n_vocab))\n",
    "\n",
    "for i in range(n_seq):\n",
    "\tx_sequence = raw_text[i * seq_length : (i + 1) * seq_length]\n",
    "\tx_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "\tfor j in range(seq_length):\n",
    "\t\tchar = x_sequence[j]\n",
    "\t\tindex = char_to_index[char]\n",
    "\t\tx_sequence_ohe[j][index] = 1.\n",
    "\tX[i] = x_sequence_ohe\n",
    "\ty_sequence = raw_text[i * seq_length + 1 : (i + 1) * seq_length + 1]\n",
    "\ty_sequence_ohe = np.zeros((seq_length, n_vocab))\n",
    "\tfor j in range(seq_length):\n",
    "\t\tchar = y_sequence[j]\n",
    "\t\tindex = char_to_index[char]\n",
    "\t\ty_sequence_ohe[j][index] = 1.\n",
    "\tY[i] = y_sequence_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww0yUi1VbGGY"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45gkRZOa4rrG",
    "outputId": "c6006182-12bc-449f-97cd-b3f1530d35cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, None, 800)         686400    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 800)         0         \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, None, 800)         1280800   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 800)         0         \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 57)         45657     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, None, 57)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,012,857\n",
      "Trainable params: 2,012,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_layer = 2\n",
    "hidden_units = 800\n",
    "n_epoch = 300\n",
    "dropout = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, activation='relu', input_shape=(None, n_vocab), return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(n_layer - 1):\n",
    "    model.add(SimpleRNN(hidden_units, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(TimeDistributed(Dense(n_vocab)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQXCTfC4auWs",
    "outputId": "1c867cba-04c0-4bd9-e41d-df27368c8836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_6 (SimpleRNN)    (None, None, 800)         686400    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 800)         0         \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, None, 800)         1280800   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, None, 800)         0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, None, 57)         45657     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, None, 57)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,012,857\n",
      "Trainable params: 2,012,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 58499200.0000\n",
      "My War and Peace:\n",
      "5 the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore t\n",
      "\n",
      "Epoch 1: loss improved from inf to 58499200.00000, saving model to weights/weights_layer_%d_hidden_%d_epoch_001_loss_58499200.0000.hdf5\n",
      "320/320 [==============================] - 83s 247ms/step - loss: 58499200.0000\n",
      "Epoch 2/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.9592\n",
      "Epoch 2: loss improved from 58499200.00000 to 1.95920, saving model to weights/weights_layer_%d_hidden_%d_epoch_002_loss_1.9592.hdf5\n",
      "320/320 [==============================] - 67s 210ms/step - loss: 1.9592\n",
      "Epoch 3/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.5303\n",
      "My War and Peace:\n",
      "he was a strange and the same that the same that the same that the same that the same that the same that the same that the same that the same that the same that the same that the same that the same tha\n",
      "\n",
      "Epoch 3: loss improved from 1.95920 to 1.53034, saving model to weights/weights_layer_%d_hidden_%d_epoch_003_loss_1.5303.hdf5\n",
      "320/320 [==============================] - 77s 242ms/step - loss: 1.5303\n",
      "Epoch 4/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.3982\n",
      "Epoch 4: loss improved from 1.53034 to 1.39825, saving model to weights/weights_layer_%d_hidden_%d_epoch_004_loss_1.3982.hdf5\n",
      "320/320 [==============================] - 67s 210ms/step - loss: 1.3982\n",
      "Epoch 5/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.3459\n",
      "My War and Peace:\n",
      "é the streets and the streets and the streets and the streets and the streets and the streets and the streets and the streets and the streets and the streets and the streets and the streets and the str\n",
      "\n",
      "Epoch 5: loss improved from 1.39825 to 1.34589, saving model to weights/weights_layer_%d_hidden_%d_epoch_005_loss_1.3459.hdf5\n",
      "320/320 [==============================] - 79s 248ms/step - loss: 1.3459\n",
      "Epoch 6/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.3047\n",
      "Epoch 6: loss improved from 1.34589 to 1.30472, saving model to weights/weights_layer_%d_hidden_%d_epoch_006_loss_1.3047.hdf5\n",
      "320/320 [==============================] - 67s 208ms/step - loss: 1.3047\n",
      "Epoch 7/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2769\n",
      "My War and Peace:\n",
      "he had been a serious and the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the so\n",
      "\n",
      "Epoch 7: loss improved from 1.30472 to 1.27693, saving model to weights/weights_layer_%d_hidden_%d_epoch_007_loss_1.2769.hdf5\n",
      "320/320 [==============================] - 77s 241ms/step - loss: 1.2769\n",
      "Epoch 8/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2568\n",
      "Epoch 8: loss improved from 1.27693 to 1.25676, saving model to weights/weights_layer_%d_hidden_%d_epoch_008_loss_1.2568.hdf5\n",
      "320/320 [==============================] - 66s 205ms/step - loss: 1.2568\n",
      "Epoch 9/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2409\n",
      "My War and Peace:\n",
      "who was a strange and the same thing that the countess was still to be a strange and the same thing that the countess was still to be a strange and the same thing that the countess was still to be a st\n",
      "\n",
      "Epoch 9: loss improved from 1.25676 to 1.24089, saving model to weights/weights_layer_%d_hidden_%d_epoch_009_loss_1.2409.hdf5\n",
      "320/320 [==============================] - 78s 243ms/step - loss: 1.2409\n",
      "Epoch 10/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2286\n",
      "Epoch 10: loss improved from 1.24089 to 1.22858, saving model to weights/weights_layer_%d_hidden_%d_epoch_010_loss_1.2286.hdf5\n",
      "320/320 [==============================] - 66s 205ms/step - loss: 1.2286\n",
      "Epoch 11/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2177\n",
      "My War and Peace:\n",
      "s and the same thing is a strange and the same thing is a strange and the same thing is a strange and the same thing is a strange and the same thing is a strange and the same thing is a strange and the\n",
      "\n",
      "Epoch 11: loss improved from 1.22858 to 1.21772, saving model to weights/weights_layer_%d_hidden_%d_epoch_011_loss_1.2177.hdf5\n",
      "320/320 [==============================] - 77s 240ms/step - loss: 1.2177\n",
      "Epoch 12/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2090\n",
      "Epoch 12: loss improved from 1.21772 to 1.20898, saving model to weights/weights_layer_%d_hidden_%d_epoch_012_loss_1.2090.hdf5\n",
      "320/320 [==============================] - 66s 207ms/step - loss: 1.2090\n",
      "Epoch 13/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2010\n",
      "My War and Peace:\n",
      "s and the same time the same thing that had been and the same thing that had been and the same thing that had been and the same thing that had been and the same thing that had been and the same thing t\n",
      "\n",
      "Epoch 13: loss improved from 1.20898 to 1.20096, saving model to weights/weights_layer_%d_hidden_%d_epoch_013_loss_1.2010.hdf5\n",
      "320/320 [==============================] - 78s 243ms/step - loss: 1.2010\n",
      "Epoch 14/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1947\n",
      "Epoch 14: loss improved from 1.20096 to 1.19465, saving model to weights/weights_layer_%d_hidden_%d_epoch_014_loss_1.1947.hdf5\n",
      "320/320 [==============================] - 66s 206ms/step - loss: 1.1947\n",
      "Epoch 15/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1892\n",
      "My War and Peace:\n",
      "0 the soldiers and the same thing that had been an angry and the same thing that had been an angry and the same thing that had been an angry and the same thing that had been an angry and the same thing\n",
      "\n",
      "Epoch 15: loss improved from 1.19465 to 1.18921, saving model to weights/weights_layer_%d_hidden_%d_epoch_015_loss_1.1892.hdf5\n",
      "320/320 [==============================] - 76s 238ms/step - loss: 1.1892\n",
      "Epoch 16/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1836\n",
      "Epoch 16: loss improved from 1.18921 to 1.18359, saving model to weights/weights_layer_%d_hidden_%d_epoch_016_loss_1.1836.hdf5\n",
      "320/320 [==============================] - 65s 203ms/step - loss: 1.1836\n",
      "Epoch 17/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1793\n",
      "My War and Peace:\n",
      "ête and the same thing to the countess was a strange and soldiers and the same thing to the countess was a strange and soldiers and the same thing to the countess was a strange and soldiers and the sam\n",
      "\n",
      "Epoch 17: loss improved from 1.18359 to 1.17928, saving model to weights/weights_layer_%d_hidden_%d_epoch_017_loss_1.1793.hdf5\n",
      "320/320 [==============================] - 74s 233ms/step - loss: 1.1793\n",
      "Epoch 18/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1759\n",
      "Epoch 18: loss improved from 1.17928 to 1.17589, saving model to weights/weights_layer_%d_hidden_%d_epoch_018_loss_1.1759.hdf5\n",
      "320/320 [==============================] - 64s 202ms/step - loss: 1.1759\n",
      "Epoch 19/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1720\n",
      "My War and Peace:\n",
      "ment of the commander-in-chief and the same time the soldiers were standing in the staff of the commander-in-chief and the same time the soldiers were standing in the staff of the commander-in-chief an\n",
      "\n",
      "Epoch 19: loss improved from 1.17589 to 1.17198, saving model to weights/weights_layer_%d_hidden_%d_epoch_019_loss_1.1720.hdf5\n",
      "320/320 [==============================] - 76s 236ms/step - loss: 1.1720\n",
      "Epoch 20/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1686\n",
      "Epoch 20: loss improved from 1.17198 to 1.16855, saving model to weights/weights_layer_%d_hidden_%d_epoch_020_loss_1.1686.hdf5\n",
      "320/320 [==============================] - 64s 201ms/step - loss: 1.1686\n",
      "Epoch 21/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1658\n",
      "My War and Peace:\n",
      "s and the same time the same time the same time the same time the same time the same time the same time the same time the same time the same time the same time the same time the same time the same time\n",
      "\n",
      "Epoch 21: loss improved from 1.16855 to 1.16584, saving model to weights/weights_layer_%d_hidden_%d_epoch_021_loss_1.1658.hdf5\n",
      "320/320 [==============================] - 75s 234ms/step - loss: 1.1658\n",
      "Epoch 22/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1627\n",
      "Epoch 22: loss improved from 1.16584 to 1.16273, saving model to weights/weights_layer_%d_hidden_%d_epoch_022_loss_1.1627.hdf5\n",
      "320/320 [==============================] - 65s 202ms/step - loss: 1.1627\n",
      "Epoch 23/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1605\n",
      "My War and Peace:\n",
      "king the sound of the streets and the same time the countess was so sorry for the countess was so sorry for the countess was so sorry for the countess was so sorry for the countess was so sorry for the\n",
      "\n",
      "Epoch 23: loss improved from 1.16273 to 1.16046, saving model to weights/weights_layer_%d_hidden_%d_epoch_023_loss_1.1605.hdf5\n",
      "320/320 [==============================] - 75s 236ms/step - loss: 1.1605\n",
      "Epoch 24/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1587\n",
      "Epoch 24: loss improved from 1.16046 to 1.15865, saving model to weights/weights_layer_%d_hidden_%d_epoch_024_loss_1.1587.hdf5\n",
      "320/320 [==============================] - 64s 200ms/step - loss: 1.1587\n",
      "Epoch 25/300\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1565\n",
      "My War and Peace:\n",
      "6 the same thing that had been a stranger and the same thing that had been a stranger and the same thing that had been a stranger and the same thing that had been a stranger and the same thing that had\n",
      "\n",
      "Epoch 25: loss improved from 1.15865 to 1.15645, saving model to weights/weights_layer_%d_hidden_%d_epoch_025_loss_1.1565.hdf5\n",
      "320/320 [==============================] - 75s 234ms/step - loss: 1.1565\n",
      "Epoch 26/300\n",
      "181/320 [===============>..............] - ETA: 28s - loss: 1.1508"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_layer = 2\n",
    "hidden_units = 800\n",
    "n_epoch = 300\n",
    "dropout = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, activation='relu', input_shape=(None, n_vocab), return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(n_layer - 1):\n",
    "    model.add(SimpleRNN(hidden_units, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(TimeDistributed(Dense(n_vocab)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "def generate_text(model, gen_length, n_vocab, index_to_char):\n",
    "    \"\"\"\n",
    "    Generating text using the RNN model\n",
    "    @param model: current RNN model\n",
    "    @param gen_length: number of characters we want to generate\n",
    "    @param n_vocab: number of unique characters\n",
    "    @param index_to_char: index to character mapping\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    # Start with a randomly picked character\n",
    "    index = np.random.randint(n_vocab)\n",
    "    y_char = [index_to_char[index]]\n",
    "    X = np.zeros((1, gen_length, n_vocab))\n",
    "    for i in range(gen_length):\n",
    "        X[0, i, index] = 1.\n",
    "        indices = np.argmax(model.predict(X[:, max(0, i - 99):i + 1, :])[0], 1)\n",
    "        index = indices[-1]\n",
    "        y_char.append(index_to_char[index])\n",
    "    return ('').join(y_char)\n",
    "\n",
    "\n",
    "class ResultChecker(Callback):\n",
    "    def __init__(self, model, N, gen_length):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.gen_length = gen_length\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.N == 0:\n",
    "            result = generate_text(self.model, self.gen_length, n_vocab, index_to_char)\n",
    "            print('\\nMy War and Peace:\\n' + result)\n",
    "\n",
    "\n",
    "filepath=\"weights/weights_layer_%d_hidden_%d_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')\n",
    "\n",
    "model.fit(X, Y, batch_size=batch_size, verbose=1, epochs=n_epoch,\n",
    "                 callbacks=[ResultChecker(model, 2, 200), checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xriNfnmcam3g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled48.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
